---
title: "Assignment 1"
author: "Kshitij Kavimandan, Pablo Alves, Pooja Mangal (Group 15)"
date: "20 February 2024"
output: pdf_document
fontsize: 11pt
highlight: tango
---

## Exercise 1. Ice cream

**a)** Relevant plots and normality assessment:

```{r}
# Load the dataset
ice_cream_dataset <- read.csv("Ice_cream.csv")
data = ice_cream_dataset$video
library("drcarlate")
```


```{r,fig.margin = TRUE,fig.width=6,fig.height=4,fig.align="centerr"}
# Do basic plots
par(mfrow=c(1,2)); hist(data); qqnorm(data); boxplot(data)
```

The histogram of the videogame scores looks normally distributed because it is roughly symetrical around 50, with values getting smaller towards its extremes. Despite this, the histogram bar for values between 60 and 65 is higher than what the corresponding analytical normal distribution should have.

Secondly, the Q-Q Plot also shows that our sample is densely populated in the center and its shape is very close to a line, which is also consistent with normality.

Finally, the boxplot is also consistent with normality, despite its median being not symetrical with its surrounding quantiles. As shown with the histogram, this can be explained with the additional values which are more frequently present in that particular region.

In short, our inspection of these basic plots suggest that this sample is normally distributed.

-----

Because we are assuming by definition that our sample is normally distributed, we can apply the 68-95-99.7% rule. Because our sample has a mean of 51.85 and a standard deviation of 9.9, we can then see that the desired 97% confidence interval for the mean should lie inside the 6 sigma interval, which has a length of 59.9. With this rough estimation, our 97% confidence value for our mean is 51.85 +- 29.7, which is akin to the [22.15,81.55] interval.

To get the 97% CI, we need to evaluate the margin of error with the formula

margin = T(0.97)*(std(x)/sqrt(N))

where T is the t distribution with N-1 degrees of freedom, N our sample size (200)...



```{r}
options(digits = 3)
n <- length(data)
margin <- qt(0.97, df=n-1)*sd(data)/sqrt(n)
min_mean = mean(data) - margin
max_mean = mean(data) + margin
length = (max_mean - min_mean)/2; length
paste("97 CI: ", mean(data), " +-", length)
```

Thus, the 97% CI for the mean is [50.52571,53.1749], with a length of 2.648579


In order to compute the bounded 97% CI for mu, we need to use the following formula:

```{r}
options(digits = 3)
m_calculator<-function(p, data) sqrt(sd(data)/length(data)*(4*norminv(p+(1-p)/2)))
margin = m_calculator(p=0.97,data=data)
min_mean = mean(data) - margin
max_mean = mean(data) + margin
length = max_mean - min_mean; length
paste("97 CI: ", mean(data)," +-", length)

```

In order to compute the neccesary number of samples needed for a particular maximun length of the CI we 
can use the following formula

```{r}
options(digits = 3)
n_calculator <- function(p,length,data) sd(data)*(2*norminv(p+(1-p)/2)/length)^2
samples_needed = n_calculator(p=0.97,length=3,data=data)
paste("For a margin of: ", 3 ," we need", samples_needed)

```


To determine the 97% bootstrap CI we can apply the following code

```{r}
options(digits = 3)
p = 0.97
B = 100

Tstar = numeric(B)
for(i in 1:B){Tstar[i]=mean(sample(data,replace=TRUE))}

Tstar985 = quantile(Tstar,0.985)
Tstar15 = quantile(Tstar, 0.015)

c(2*mean(Tstar) - Tstar985,2*mean(Tstar)-Tstar15)

d = ((2*mean(Tstar)-Tstar15) - (2*mean(Tstar) - Tstar985))
paste("Length:",d)
paste("Margin on each side:",d/2)

```

Our Bootstrap margin is bigger than our previous margins, which is consistent with the fact that the Bootstrap method does not assume normality, and thus is expected to have a bigger uncertainty on its estimation.

-----

**b)** T-test to verify mean score:

The t-test compares the mean of the sample to the hypothesized mean and calculates a t-statistic and a p-value.

```{r}
options(digits = 3)
mu = 50
x = rnorm(length(data), mu, 1)
par(mfrow=c(1,2))
hist(x)
boxplot(x)
t_test_result <- t.test(x, alternative="g", mu = mu)
print(t_test_result)
t_test_result_mu_51 <- t.test(x, alternative="g", mu = 51)
t_test_result_mu_51
```

- In the first test, with mu_0 = 50, the p-value is more than the significance level of 0.05 which suggests that there isn't enough evidence to reject the null hypothesis, indicating that the mean score on the video game is not significantly different from 50. 

- Similarly, when testing against mu_0 = 51, the high p-value of 1 implies strong evidence in favor of the null hypothesis, meaning that the mean score is not significantly greater than 51.

- In the output, the 95% confidence interval for the first test is (49.9, infinity). This means that we are 95% confident that the true population mean lies in the above interval. The upper bound of infinity indicates that the upper limit is unbounded.

- The confidence interval for the second test is the same, as it is based on the sample mean and standard deviation. It doesn't change with the hypothesized mean mu_0.

**c)** Sign test and test based on ranks:

```{r}
options(digits = 3)
# Sign test
sign_test <- binom.test(sum(data > 50), length(data), p = 0.5, alternative = "g")
print(sign_test)

# Test based on ranks (Wilcoxon signed-rank test)
wilcox_test <- wilcox.test(data, mu = 50, alternative = "g")
print(wilcox_test)

# Test for fraction of scores less than 42
binom_test <- binom.test(sum(data < 42), length(data), p = 0.25, alternative = "l")
print(binom_test)
```


- For the median score, the sign test results in a p-value of 0.144, suggesting no significant difference between the median score and mu_0 = 50. The 95% confidence interval for the success probability of the sign test ranges from 0.479 to 1.

- Comments on comparison of the above results with the previous question (b)):
*The t-test evaluates whether the mean score is significantly greater than 50, while the sign test assesses if the median score is greater than 50. Both tests yield p-values greater than 0.05, indicating that there's insufficient evidence to reject the null hypotheses. The confidence interval from the t-test provides a range of plausible values for the population mean, whereas the sign test directly assesses the proportion of observations above 50.*

- The Wilcoxon signed-rank test results in a p-value of 0.005, indicating a significant difference between the median score and mu_0 = 50. The alternative hypothesis suggests that the true location is greater than 50.

- Lastly, the binomial test to check the fraction of scores less than 42 yields a p-value of 0.0007, suggesting that the probability of success (scores less than 42) is less than 0.25. 

**d)** Bootstrap test with test statistic T:

```{r}
options(digits = 3)
# Bootstrap test (slides)
n = length(data); t=min(data); t
B = 100; tstar = numeric(B)
for (i in 1:B) {xstar=rexp(n,1)
 tstar[i]=min(xstar)}
pl = sum(tstar<t)/B; pr=sum(tstar>t)/B
p=2*min(pl,pr); p



# Bootstrap test (google)
# Define the test statistic function
test_statistic <- function(data) {
  return(min(data))
}

# Number of bootstrap samples
B <- 1000

# Bootstrap test
bootstrap_test <- function(mu) {
  tstar <- numeric(B)
  for (i in 1:B) {
    bootstrap_sample <- rnorm(B, mean = mu, sd = 10)  # Generate bootstrap sample
    tstar[i] <- test_statistic(bootstrap_sample)       # Compute minimum
  }
  
  # Compute observed minimum
  obs_min <- test_statistic(data[1:100])
  
  # Compute p-value
  p_value <- mean(tstar >= obs_min)
  
  # Determine if null hypothesis is rejected
  count <- 0
  if (p_value > 0.05) {
    reject_null <- TRUE
  } else {
    reject_null <- FALSE
    count <- count + 1
  }
  
  return(list(mu = mu, p_value = p_value, reject_null = reject_null))
}

# Perform bootstrap test for different values of mu in the range [0, 100]
results <- lapply(seq(0, 100, by = 1), bootstrap_test)

# Display results
for (result in results) {
  cat("mu:", result$mu, "p-value:", result$p_value, "Reject H0:", result$reject_null, "\n")
}



# Apply Kolmogorov-Smirnov test
# Perform Kolmogorov-Smirnov test for different values of mu in the range [0, 100]
mu_values_not_rejected <- c()

for (mu in seq(0, 100, by = 1)) {
  # Generate the first 100 samples from N(mu, 100)
  sample_data <- rnorm(100, mean = mu, sd = 10)
  
  # Compute the Kolmogorov-Smirnov statistic and p-value
  ks_test_result <- ks.test(sample_data, "pnorm", mean = mu, sd = 10)
  
  # Check if null hypothesis is rejected
  if (ks_test_result$p.value > 0.05) {
    mu_values_not_rejected <- c(mu_values_not_rejected, mu)
  }
}

# Display mu values for which null hypothesis is not rejected
cat("Mu values for which null hypothesis is not rejected:", mu_values_not_rejected, "\n")

```

Since p-value=0.007, H0 is rejected.

```{r}
hist(tstar,prob=T, main="Histogram of tstar")
lines(rep(t,2),c(0,p), col="red",lwd=2)
axis(1,t,expression(paste("t")))
```


**e)** Tests for male and female students:

```{r}
options(digits = 3)
data <- read.csv("Ice_cream.csv")

# Separate scores for male and female students
male_scores <- data$video[data$female == 0]
female_scores <- data$video[data$female == 1]
# Perform two-sample t-test
t_test_gender <- t.test(male_scores, female_scores, alternative = "g"); t_test_gender
# Perform Mann-Whitney test
mannwhitney_test <- wilcox.test(male_scores, female_scores,
alternative = "g"); mannwhitney_test
# Perform Kolmogorov-Smirnov test
ks_test_gender <- ks.test(male_scores, female_scores); ks_test_gender

```

**f)** Correlation and comparison between video game and puzzle scores:

```{r}
options(digits = 3)
# Investigate correlation
correlation <- cor(data$video, data$puzzle); correlation
# Test if puzzle scores are higher than video game scores
wilcox_test_puzzle <- wilcox.test(data$puzzle, data$video, alternative = "g"); wilcox_test_puzzle

```

## Exercise 2. Hemoglobin in trout
**a)** R-code for randomization process:
a) R-code for randomization process:
```{r}
options(digits = 3)
# Load the data
hemoglobin_data <- read.table("hemoglobin.txt", header = TRUE)
# Create a data frame to store the combinations of rate and method
combinations <- expand.grid(rate = c(1, 2, 3, 4), method = c("A", "B"))
# Randomly assign 80 fishes to the combinations
set.seed(123) # Set seed for reproducibility
hemoglobin_data$fish <- sample(rep(1:80,
length.out = nrow(hemoglobin_data)),
replace = FALSE)
# Merge the combinations with the hemoglobin data
final_data <- merge(combinations, hemoglobin_data, by = "rate", all = TRUE)
``` 

**b)** Two-way ANOVA:

```{r}
options(digits = 3)
# Perform two-way ANOVA
anova_result <- aov(hemoglobin ~ rate * method, data = hemoglobin_data)
# Print ANOVA table
print(summary(anova_result))
```

**c)** Influence of factors and combination yielding highest hemoglobin:
```{r}
options(digits = 3)
# Calculate means for rate and method
mean_hemoglobin <- aggregate(hemoglobin ~ rate + method,
data = hemoglobin_data, FUN = mean)
# Find combination yielding highest hemoglobin
max_hemoglobin <- mean_hemoglobin[which.max(mean_hemoglobin$hemoglobin), ]
# Estimate mean hemoglobin value for rate 3 by using method A
mean_hemoglobin_rate3_methodA <-
mean_hemoglobin$hemoglobin[which(mean_hemoglobin$rate == 3 &
mean_hemoglobin$method == "A")]
# Estimate mean hemoglobin value for each rate
mean_hemoglobin_rate <- aggregate(hemoglobin ~ rate,
data = hemoglobin_data, FUN = mean)
```

**d)** One-way ANOVA:

```{r}
options(digits = 3)
# Perform one-way ANOVA ignoring the variable method
anova_result_rate <- aov(hemoglobin ~ rate, data = hemoglobin_data)
# Print ANOVA table
print(summary(anova_result_rate))
```

**e)** Kruskal-Wallis test:
```{r}
options(digits = 3)
# Perform Kruskal-Wallis test
kruskal_test_result <- kruskal.test(hemoglobin ~ rate, data = hemoglobin_data)
# Print Kruskal-Wallis test result
print(kruskal_test_result)
```

# Exercise 3. Sour cream


Read the data 
Columns: acidity batch position starter

```{r}
data <- read.table(file = "cream.txt", header = TRUE, sep="", dec=".")
```

**a)** Analyzing the data in a three-way experiment without interactions:


I decide to do 3-way additive ANOVA test, for it does not assume interaction (notice the + and not *)
```{r}
model <- aov(acidity ~ starter + batch + position, data=data)
summary(model)
```

## Our test is intended to test the isolated effect of different factors
## Because for our starter factor we have a Pr(>F) of 0.431, 
## which is big >> 0.05, we cannot conclude that there is a significant difference
## between the effects of starter 1 and starte 2 on acidity

## Instead, we could do a two-sample t-test to see
## if the average acidity changes significantly with starter 1 or 2,
## which shows a p-value of 0.8482, and thus we can't reject H0
## so in summary we can't say that there is a significant 
## difference between the effects of starter 1 and starter 2 on acidity?

```{r}
sample1 = data[data$starte == 1, ]
sample2 = data[data$starte == 2, ]
t.test(sample1$acidity, sample2$acidity, var.equal=TRUE) 
```

## EXTRA If we really wanted to go further we could also do an interaction plot,
## then we can see that for lines of starter 1 and 2
## Like this: interaction.plot(data$acidity,data$starter,data$batch)

**b)** Removing insignificant block variables and performing ANOVA:


## To find insignificant block variables and thus remove them
## we do ANOVA again, but this time ithout starter

```{r}
model2 <- aov(acidity ~ batch + position, data=data)
summary(model2)
```

## Here both variables have still a big p, so while technically
## all variables could be discarded, which would be absurd,
## we could just remove position, for its p-value 0.788
## is greater than the 0.222 value of batch, despite both
## being big.

## Thus, to find which starters lead to significantly different acidity
## we could to an interaction plot like this
interaction.plot(data$acidity,data$starter,data$batch)



**c)** Applying the Friedman test:

## Because we have severall batches for each starter position and not one
## We can't use the Friedman test

## However, if we modify our data by averaging the acidity of every batch
## to have one element per starter sample, we could do it.
## Alternatively, we could also do the Friedman test five times, one for every batch

## Using https://search.r-project.org/R/refmans/stats/html/friedman.test.html
## The syntax should be something like this (we also need to filter the extra samples)

```{r}
# friedman_test(data, acidity ~ starter)
# friedman.test(acidity ~ starter, data = data)
```



**d)**  Performing a mixed effects analysis:

## Install and load library
```{r}
library("devtools"); 
install_github("lme4/lme4",dependencies=TRUE)
library("lmer4")
```

## Getting error Error: Failed to install 'lme4' from GitHub:
## Could not find tools necessary to compile a package
## Call pkgbuild::check_build_tools(debug = TRUE) to diagnose the problem.

## Syntax will be something like this
## https://www.rdocumentation.org/packages/lme4/versions/1.1-35.1/topics/lmer

```{r}
model <- lmer(acidity ~ starter + (1|position), data = data)
```